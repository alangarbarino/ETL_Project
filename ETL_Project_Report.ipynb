{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT ETL - Extract, Translate, Load\n",
    "* Team: Kamil Borowik, Jeff Brown, Eugenio Gallastegui, Alan Garbarino\n",
    "* Date: 2/23/19\n",
    "* GitHub Repository: https://github.com/alangarbarino/ETL_Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "**Our goal was to perform Extract, Transform, Load operations that would allow an analyst to examine the Null hypothesis:**\n",
    "<br>\n",
    "    _**There is no correlation between incidents of food poisoning (as reported on source site) and results/ timing of City of Chicago health inspections**_\n",
    "    \n",
    "<br>\n",
    "\n",
    "**Sources from which the Data was Extracted:**\n",
    "\n",
    "* https://iwaspoisoned.com/#\n",
    "    * Description: I was Poisoned website, providing self-reported individual incidents of perceived food poisoning\n",
    "    * Data: Web page providing food poisoning incident date, symptoms, business in HTML format\n",
    "    * Extraction approach: Web scraping of the incidents and associated details, with breakdown of address components, and make the data available in a MongoDB database collection\n",
    "<br><br>\n",
    "* https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5\n",
    "    * Description: City of Chicago, Department of Health and Human Services, Food Inspections API \n",
    "    * Data: API endpoint providing Business, type of business, address, location, license, date, type of inspection, results in JSON format\n",
    "    * Extraction approach: Use of website API endpoint to retrieve inspection data, with cleaning to adjust data formating and data types, and make data available in a Pandas DataFrame and Sqlite database table\n",
    "<br><br>\n",
    "* https://www.latlong.net/\n",
    "    * Description: Translate address information into longitude/latitude\n",
    "    * Data - Input: Address; Output: Longitude, Latitude\n",
    "    \n",
    "<br>\n",
    "\n",
    "**Type of Transformation Performed:** -- (cleaning, joining, filtering, aggregating, etc).\n",
    "\n",
    "* TBD\n",
    "* TBD\n",
    "\n",
    "<br>    \n",
    "**Type of Final Production Database and Associated Tables/Collections:** -- (relational or non-relational)\n",
    "\n",
    "* Sqlite database `TBD` with table `TBD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Requirements  **_[DELETE THIS]_**\n",
    "## Data Cleanup & Analysis\n",
    "\n",
    "Once you have identified your datasets, perform ETL on the data. Make sure to plan and document the following:\n",
    "* The sources of data that you will extract from.\n",
    "* The type of transformation needed for this data (cleaning, joining, filtering, aggregating, etc).\n",
    "* The type of final production database to load the data into (relational or non-relational).\n",
    "* The final tables or collections that will be used in the production database.\n",
    "\n",
    "You will be required to submit a final technical report with the above information and steps required to reproduce your ETL process.\n",
    "\n",
    "\n",
    "## Project Report\n",
    "\n",
    "At the end of the week, your team will submit a Final Report that describes the following:\n",
    "* Extract: your original data sources and how the data was formatted (CSV, JSON, MySQL, etc).\n",
    "* Transform: what data cleaning or transformation was required.\n",
    "* Load: the final database, tables/collections, and why this was chosen.\n",
    "\n",
    "Please upload the report to Github and submit a link to Bootcampspot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Data Pipeline\n",
    "The full ETL data pipeline for merging our food poisoning and restaurant inspection data can be implemented using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract restaurant inspections data from City of Chicago TBD API\n",
    "# and store in a MongoDB database TBD collection TBD\n",
    "#from TBD import TBD\n",
    "\n",
    "# Extract\n",
    "# inspections_complete_df = api_to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract incidents of food poisoning from the iwaspoisoned.com (IWP) website\n",
    "# and store in a MongoDB database etl_db collection iwp\n",
    "from etl_scrape_iwp import scrape_iwp\n",
    "\n",
    "# Starting on website page 1 and scrape 20000 pages by defaults\n",
    "#npages = scrape_iwp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data by reading the extracted data from MongoDB, \n",
    "# merging the data to allow analysis based upon correlations in\n",
    "# incidents of food poisoning and history of food inspections in the\n",
    "# associated restaurants.  Return the merged DataFrame.\n",
    "#from TBD import TBD\n",
    "\n",
    "# Transform\n",
    "#TBD_df = transform_TBD(inspections_complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged DataFrame in a MySQL database for use\n",
    "# for later analysis\n",
    "#from TBD import TBD\n",
    "\n",
    "# Load\n",
    "#load_TBD( TBD_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TECHNICAL DESCRIPTION: Extract - Scraping iwaspoisoned.com\n",
    "* This Python code performs scraping of the iwaspoisoned.com website, capturing incident information on each page and across multiple pages\n",
    "* The information scraped in inserted into a MongoDB database `etl_db` in the collection `iwp` using the dictionary/document format provided below.\n",
    "* The code is structure as a set of functions:\n",
    "    * scrape_iwp(a_startpage=1, a_pagecount=20000): The function to be called by external programs to perform scraping and provide results in MongoDB.  The arguments are optional and the defaults values are as indicated.\n",
    "    \n",
    "    * Supporting functions:\n",
    "        * parse_incident_page(): This function is called for each iwaspoisoned.com website.  I uses the functions parse_one_incident() to capture the required information for each incident and returns an array of dictionaries.\n",
    "        * parse_one_incident(): This function is called for each incident and captures needed per-incident data, including a call to get_incident_details() to scrape incident address information from the per-incident detail page.\n",
    "        * get_incident_detail(): This function is called on a per-incident basis to navigate to the incident detail page to collect information not available from the main page, specifically: address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Format\n",
    "The documents in the etl_db collection iwp have the following formats:\n",
    "1. United States - Address is parsed\n",
    "<br>\n",
    "\n",
    "```{'_id': ObjectId('5c6a8db6aaa45be6f85938c7'),\n",
    " 'incident_title': \"BJ's Restaurant & Brewhouse, Red Bug Lake Road, Oviedo, FL, USA\",\n",
    " 'incident_date': 'Dec 30 2018 10:51pm',\n",
    " 'incident_url': 'https://iwaspoisoned.com/incident/bjs-restaurant-brewhouse-red-bug-lake-road-oviedo-fl-usa-155865#emailscroll',\n",
    " 'incident_report_type': 'Food Poisoning',\n",
    " 'incident_symptoms': ['Diarrhea', 'Nausea', 'Vomiting'],\n",
    " 'incident_misc': '',\n",
    " 'incident_address_info':\n",
    "     {'incident_address': '8285 Red Bug Lake Road, Oviedo, 32765 Florida, United States',\n",
    "      'incident_address_standard': '8285 Red Bug Lake Road, Oviedo, Florida 32765, United States',\n",
    "      'incident_address_street': '8285 Red Bug Lake Road',\n",
    "      'incident_address_street2': '',\n",
    "      'incident_address_street3': '',\n",
    "      'incident_address_city': 'Oviedo',\n",
    "      'incident_address_state': 'Florida',\n",
    "      'incident_address_zipcode': '32765',\n",
    "      'incident_address_country': 'United States'},\n",
    " 'incident_description': 'Ate there Friday evening, got sick about 8 hours later'}```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "2. Other Countries - Address is not parsed\n",
    "(Should only USA entries in the database due to filtering, but just in case...)\n",
    "<br>\n",
    "\n",
    "```{'_id': ObjectId('5c6a8db6aaa45be6f85938c7'),\n",
    " 'incident_title': \"BJ's Restaurant & Brewhouse, Red Bug Lake Road, Oviedo, FL, USA\",\n",
    " 'incident_date': 'Dec 30 2018 10:51pm',\n",
    " 'incident_url': 'https://iwaspoisoned.com/incident/bjs-restaurant-brewhouse-red-bug-lake-road-oviedo-fl-usa-155865#emailscroll',\n",
    " 'incident_report_type': 'Food Poisoning',\n",
    " 'incident_symptoms': ['Diarrhea', 'Nausea', 'Vomiting'],\n",
    " 'incident_misc': '',\n",
    " 'incident_address_info':\n",
    "     {'incident_address': '8285 Red Bug Lake Road, Oviedo, 32765 Florida, United States'},\n",
    " 'incident_description': 'Ate there Friday evening, got sick about 8 hours later'}```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Scraping IWP\n",
    "Scraped data from 6508 pages on the `iwaspoisoned.com` website for dates 4/1/2017 through 2/19/2019\n",
    "* 57794 incident reports for USA\n",
    "* 2542 incident reports for Illinois ==> This was used for Transform/Load steps below\n",
    "* 780 incident reports for Chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Scraping IWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# PROJECT ETL\n",
    "#@Author: Jeff Brown (daddyjab)<br>\n",
    "#@Date: 2/21/19<br>\n",
    "#@GitHub Repository: https://github.com/alangarbarino/ETL_Project\n",
    "\n",
    "\n",
    "# EXTRACT: SCRAPING iwaspoisoned.com\n",
    "#* This Python code performs scraping of the iwaspoisoned.com website, capturing incident information on each page and across multiple pages\n",
    "#* The information scraped in inserted into a MongoDB database `etl_db` in the collection `iwp` using the dictionary/document format provided below.\n",
    "#* The code is structure as a set of functions:\n",
    "#    * scrape_iwp(a_startpage=1, a_pagecount=20000): The function to be called by external programs to perform scraping and provide results in MongoDB.  The arguments are optional and the defaults values are as indicated.\n",
    "#    \n",
    "#    * Supporting functions:\n",
    "#        * parse_incident_page(): This function is called for each iwaspoisoned.com website.  I uses the functions parse_one_incident() to capture the required information for each incident and returns an array of dictionaries.\n",
    "#        * parse_one_incident(): This function is called for each incident and captures needed per-incident data, including a call to get_incident_details() to scrape incident address information from the per-incident detail page.\n",
    "#        * get_incident_detail(): This function is called on a per-incident basis to navigate to the incident detail page to collect information not available from the main page, specifically: address.\n",
    "        \n",
    "\n",
    "# HOW THE `iwp` COLLECTION WAS EXPORTED\n",
    "#The `iwp` collection in the `etl_db` database was exported to the file `mongodb_export_iwp.json` using:<br>\n",
    "#```mongoexport --db etl_db --collection iwp --out mongodb_export_iwp.json```\n",
    "#\n",
    "#Later, `iwp` was filtered for only Illinois incidents, which were then stored in another collection `iwp_illinois_only`.  This collection was exported using:<br>\n",
    "#```mongoexport --db etl_db --collection iwp_illinois_only --out mongodb_export_iwp.json```\n",
    "\n",
    "\n",
    "# HOW THE `iwp` COLLECTION CAN BE IMPORTED \n",
    "#The JSON can be imported to the `iwp` collection in the `etl_db` database from file `mongodb_export_iwp.json` using:<br>\n",
    "#```mongoimport --db etl_db --collection iwp --file mongodb_export_iwp.json```\n",
    "\n",
    "\n",
    "# DOCUMENT FORMAT\n",
    "#The documents in the etl_db collection iwp have the following formats:\n",
    "#1. United States - Address is parsed\n",
    "#<br>\n",
    "#\n",
    "#```{'_id': ObjectId('5c6a8db6aaa45be6f85938c7'),\n",
    "# 'incident_title': \"BJ's Restaurant & Brewhouse, Red Bug Lake Road, Oviedo, FL, USA\",\n",
    "# 'incident_date': 'Dec 30 2018 10:51pm',\n",
    "# 'incident_url': 'https://iwaspoisoned.com/incident/bjs-restaurant-brewhouse-red-bug-lake-road-oviedo-fl-usa-155865#emailscroll',\n",
    "# 'incident_report_type': 'Food Poisoning',\n",
    "# 'incident_symptoms': ['Diarrhea', 'Nausea', 'Vomiting'],\n",
    "# 'incident_misc': '',\n",
    "# 'incident_address_info':\n",
    "#     {'incident_address': '8285 Red Bug Lake Road, Oviedo, 32765 Florida, United States',\n",
    "#      'incident_address_standard': '8285 Red Bug Lake Road, Oviedo, Florida 32765, United States',\n",
    "#      'incident_address_street': '8285 Red Bug Lake Road',\n",
    "#      'incident_address_street2': '',\n",
    "#      'incident_address_street3': '',\n",
    "#      'incident_address_city': 'Oviedo',\n",
    "#      'incident_address_state': 'Florida',\n",
    "#      'incident_address_zipcode': '32765',\n",
    "#      'incident_address_country': 'United States'},\n",
    "# 'incident_description': 'Ate there Friday evening, got sick about 8 hours later'}```\n",
    "#\n",
    "#<br>\n",
    "\n",
    "\n",
    "#2. Other Countries - Address is not parsed\n",
    "#(Should only USA entries in the database due to filtering, but just in case...)\n",
    "#<br>\n",
    "#\n",
    "#```{'_id': ObjectId('5c6a8db6aaa45be6f85938c7'),\n",
    "# 'incident_title': \"BJ's Restaurant & Brewhouse, Red Bug Lake Road, Oviedo, FL, USA\",\n",
    "# 'incident_date': 'Dec 30 2018 10:51pm',\n",
    "# 'incident_url': 'https://iwaspoisoned.com/incident/bjs-restaurant-brewhouse-red-bug-lake-road-oviedo-fl-usa-155865#emailscroll',\n",
    "# 'incident_report_type': 'Food Poisoning',\n",
    "# 'incident_symptoms': ['Diarrhea', 'Nausea', 'Vomiting'],\n",
    "# 'incident_misc': '',\n",
    "# 'incident_address_info':\n",
    "#     {'incident_address': '8285 Red Bug Lake Road, Oviedo, 32765 Florida, United States'},\n",
    "# 'incident_description': 'Ate there Friday evening, got sick about 8 hours later'}```\n",
    "#\n",
    "#<br>\n",
    "\n",
    "# Extract Data from iwaspoisoned.com website using web scraping.\n",
    "# Then populate the information in a MongoDB\n",
    "# (to facilitate teaming, export the MongoDB to a JSON file)\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pprint import pprint\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "\n",
    "import time\n",
    "\n",
    "# Support export to and import from JSON file\n",
    "import json\n",
    "\n",
    "# ************************************************************************************\n",
    "# FUNCTION: get_incident_detail()\n",
    "# This function accepts a url that points to a single incident detail page\n",
    "# and returns a dictionary with info from that page.\n",
    "#\n",
    "# Note: This function performs special parsing of addresses in the United States.\n",
    "# Addresses in the US will be parsed down to individual components\n",
    "# (street, street2, street3, city, state, zipcode, country)\n",
    "# In addition, these parsed components are then recombined\n",
    "# to form the full address in \"standard\" format\n",
    "# (i.e., Zipcode after the state instead of before state)\n",
    "#\n",
    "# Addresses for other countries are provided only as an address string\n",
    "#\n",
    "# Arguments:\n",
    "#    incident_detail_url: URL of the incident detail page\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def get_incident_detail(a_url):\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll'\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/subway-terminal-3-silver-dart-drive-toronto-on-canada-168642#emailscroll'\n",
    "    if len(a_url) == 0:\n",
    "        return None\n",
    "    \n",
    "    url_incident = a_url\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url_incident)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='single-incident')\n",
    "\n",
    "    for r in results:\n",
    "        # Incident detail page - title\n",
    "        # incident_detail_title = r.find('h1', class_='h1 post-title').text.strip()\n",
    "\n",
    "        # Address\n",
    "        try:\n",
    "            addr_info = r.find('span', class_='pl-1 py-0 text-muted').text.strip()\n",
    "            incident_address = ' '.join(addr_info.split())\n",
    "            \n",
    "        except AttributeError:\n",
    "            addr_info = \"\"\n",
    "            incident_address = \"\"\n",
    "\n",
    "        # Ok, we now have an address of the form:\n",
    "        # 2360 North Fairfield Road, Beavercreek, 45431 Ohio, United States\n",
    "        # But, would be nice to be able to break this up into\n",
    "        # individual components to facilitate address matching,\n",
    "        # Especially with the non-standard location of the zipcode\n",
    "        if \"United States\" in incident_address:\n",
    "            # Create a list of address items\n",
    "            ai_list = incident_address.split(',')\n",
    "\n",
    "            # Some items are mandatory and are at the end of the list of length = N\n",
    "            # N-1: Country e.g. \"United States\"\n",
    "            # N-2: Zipcode and State e.g. \"45431 Ohio\"\n",
    "            # N-3: City\n",
    "            # Other entries 0 to N-4: Street/Apt/etc.\n",
    "\n",
    "            # Get the count of how many components are in the address\n",
    "            ai_size = len( ai_list )\n",
    "            \n",
    "            # Country\n",
    "            try:\n",
    "                incident_address_country = ai_list[ai_size-1].strip()\n",
    "            except IndexError:\n",
    "                incident_address_country = \"\"\n",
    "\n",
    "            # Split the next entry to get state and zipcode\n",
    "            try:\n",
    "                zs_info = ai_list[ai_size-2].strip()\n",
    "                zs_delim = zs_info.find(' ')\n",
    "                # print(f\"zs_delim: {zs_delim}, zs_info: {zs_info}\")\n",
    "                incident_address_zipcode = zs_info[:zs_delim].strip()\n",
    "                incident_address_state = zs_info[zs_delim:].strip()\n",
    "\n",
    "            except IndexError:\n",
    "                incident_address_zipcode = \"\"\n",
    "                incident_address_state = \"\"\n",
    "                \n",
    "            # City\n",
    "            try:\n",
    "                incident_address_city = ai_list[ai_size-3].strip()\n",
    "                \n",
    "            except IndexError:\n",
    "                incident_address_city = \"\"\n",
    "\n",
    "            # Process up to 3 \"street\" type entries\n",
    "            incident_address_street = \"\"\n",
    "            incident_address_street2 = \"\"\n",
    "            incident_address_street3 = \"\"\n",
    "\n",
    "            # print(f\"ai_size: {ai_size}\")\n",
    "            # First street address item\n",
    "            if ai_size >= 4:\n",
    "                try:\n",
    "                    incident_address_street = ai_list[0].strip()\n",
    "                except:\n",
    "                    incident_address_street = \"\"\n",
    "\n",
    "            # Second street address item\n",
    "            if ai_size >= 5:\n",
    "                try:\n",
    "                    incident_address_street2 = ai_list[1].strip()\n",
    "\n",
    "                except:\n",
    "                    incident_address_street2 = \"\"\n",
    "\n",
    "            # Third street address item\n",
    "            if ai_size >= 6:\n",
    "                try:\n",
    "                    incident_address_street3 = ai_list[i].strip()\n",
    "                \n",
    "                except:\n",
    "                    incident_address_street3 = \"\"\n",
    "\n",
    "            # Rebuild the address - with standard state then zipcode formating\n",
    "            incident_address_standard = incident_address_street\n",
    "            \n",
    "            if len(incident_address_street2) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street2\n",
    "            if len(incident_address_street3) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street3\n",
    "            if len(incident_address_city) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_city\n",
    "            if len(incident_address_state) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_state\n",
    "            if len(incident_address_zipcode) > 0:\n",
    "                incident_address_standard += \" \" + incident_address_zipcode\n",
    "            if len(incident_address_country) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_country\n",
    "\n",
    "\n",
    "            #print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            #print(f\">>> Incident Detail - Address - Standard: {incident_address_standard}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street: {incident_address_street}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street2: {incident_address_street2}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street3: {incident_address_street3}\")\n",
    "            #print(f\">>> Incident Detail - Address - City: {incident_address_city}\")\n",
    "            #print(f\">>> Incident Detail - Address - State: {incident_address_state}\")\n",
    "            #print(f\">>> Incident Detail - Address - Zipcode: {incident_address_zipcode}\")\n",
    "            #print(f\">>> Incident Detail - Address - Country: {incident_address_country}\")\n",
    "            #print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "                'incident_address_standard': incident_address_standard,\n",
    "                'incident_address_street': incident_address_street,\n",
    "                'incident_address_street2': incident_address_street2,\n",
    "                'incident_address_street3': incident_address_street3,\n",
    "                'incident_address_city': incident_address_city,\n",
    "                'incident_address_state': incident_address_state,\n",
    "                'incident_address_zipcode': incident_address_zipcode,\n",
    "                'incident_address_country': incident_address_country\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Ok, for other countries, don't try to parse the incident_address\n",
    "            # print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            # print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "            }\n",
    "\n",
    "        # pprint(detail_post_item)\n",
    "        \n",
    "        return detail_post_item\n",
    "    \n",
    "\n",
    "\n",
    "# ************************************************************************************\n",
    "# FUNCTION: parse_one_incident()\n",
    "# This function accepts a Beautiful Soup object that contains a single incident\n",
    "# and returns a dictionary with info for that incident.\n",
    "# This includes a call to the get_incident_detail() function,\n",
    "# which gets needed information from the detail page for this incident\n",
    "#\n",
    "# Arguments:\n",
    "#    a_bsobj: A Beautiful Soup object containing a single incident\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def parse_one_incident(a_bsobj):\n",
    "    \n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    r = a_bsobj\n",
    "\n",
    "    # Get the primary incident report info from the main box\n",
    "    main_box = r.find('div', class_='report-first-box')\n",
    "    \n",
    "    # Date the incident occurred\n",
    "    try:\n",
    "        incident_date = main_box.find('p', class_ = 'report-date').text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        incident_date = \"\"\n",
    "        \n",
    "\n",
    "    # Title of the incident\n",
    "    try:\n",
    "        incident_title = main_box.find('a')['title']\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_title = \"\"\n",
    "    \n",
    "    # Remove the tag phrase from the title if it's present\n",
    "    if \"- Got Food Poisoning? Report it now\" in incident_title:\n",
    "        i_delim = incident_title.find(\"- Got Food Poisoning? Report it now\")\n",
    "        incident_title = incident_title[:i_delim].strip()\n",
    "\n",
    "    # URL of the per-incident details\n",
    "    try:\n",
    "        incident_url = main_box.find('a')['href'].strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_url = \"\"\n",
    "\n",
    "    # Get the Symptoms\n",
    "    report_tags = main_box.find_all('p', class_ = 'report-tag')\n",
    "\n",
    "    # Parse each report tag into its proper field\n",
    "    incident_symptoms = \"\"\n",
    "    incident_report_type = \"\"\n",
    "    incident_misc = \"\"\n",
    "\n",
    "    for rt in report_tags:\n",
    "        # Get the text in this tag\n",
    "        rt_info = rt.text.strip()\n",
    "\n",
    "        # Symptoms\n",
    "        if \"Symptoms:\" in rt_info:\n",
    "            incident_symptoms = [ s.replace(',','') for s in rt_info[len(\"Symptoms: \"):].split() ]\n",
    "\n",
    "        # Report Type\n",
    "        elif \"Report Type:\" in rt_info:\n",
    "            incident_report_type = rt_info[len(\"Report Type: \"):]\n",
    "\n",
    "        # Ok... no idea what this report tag contains\n",
    "        else:\n",
    "            incident_misc = rt_info\n",
    "\n",
    "    #pprint(main_box)\n",
    "    #print(f\">>> Incident Date: {incident_date}\")\n",
    "    #print(f\">>> Incident Title: {incident_title}\")\n",
    "    #print(f\">>> Incident URL: {incident_url}\")\n",
    "    #print(f\">>> Incident Report Type: {incident_report_type}\")\n",
    "    #print(f\">>> Incident Symptoms: {incident_symptoms}\")\n",
    "    #print(f\">>> Incident Misc Info: {incident_misc}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Get the full description of the incident\n",
    "    # Assume this couple be populated in multiple paragraphs\n",
    "    desc_box = r.find('div', class_='report-second-box')\n",
    "    desc_list = desc_box.find_all('p')\n",
    "    incident_description = \"\"\n",
    "    for d in desc_list:\n",
    "        incident_description += d.text.strip()\n",
    "\n",
    "    #pprint(descbox)\n",
    "    #print(f\">>> Description: {incident_description}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Go to the detail page to get the one piece of info we\n",
    "    # need that's not on the main page - the address!\n",
    "    incident_address_info = get_incident_detail(incident_url)\n",
    "\n",
    "    # Place all this good info into a dictionary\n",
    "    post_item = {\n",
    "        'incident_title': incident_title,\n",
    "        'incident_date': incident_date,\n",
    "        'incident_url': incident_url,\n",
    "        'incident_report_type': incident_report_type,\n",
    "        'incident_symptoms': incident_symptoms,\n",
    "        'incident_misc': incident_misc,\n",
    "        'incident_address_info': incident_address_info,\n",
    "        'incident_description': incident_description\n",
    "    }\n",
    "    #pprint(post_item)\n",
    "\n",
    "    return post_item\n",
    "\n",
    "\n",
    "# ************************************************************************************\n",
    "# FUNCTION: parse_incident_page()\n",
    "# This function accepts an HTML string from an\n",
    "# IWP website page that contains multiple incidents.\n",
    "# It then loops through the incidents on the page and the uses parse_one_incident()\n",
    "# function to grab the relevant incident info from the page.\n",
    "#\n",
    "# NOTE: The incidents are filtered to keep only those that occurred in the USA\n",
    "# since our project is focused on Chicago, IL.\n",
    "#\n",
    "# Arguments:\n",
    "#    a_html: A string of HTML content containing multiple incidents\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A list of dictionaries of USA incident information\n",
    "\n",
    "def parse_incident_page(a_html):\n",
    "    \n",
    "    # Do a basic check\n",
    "    if len(a_html) == 0:\n",
    "        return None\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(a_html, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='row div-report-box')\n",
    "\n",
    "    # Keep track of how many entries we've added\n",
    "    n_incidents = 0\n",
    "\n",
    "    # Get info for all of the incidents on this page\n",
    "    incident_list = []\n",
    "    try:\n",
    "        for r in results:\n",
    "\n",
    "            # Parse this incident\n",
    "            incident_info = parse_one_incident(r)\n",
    "            #pprint(incident_info)\n",
    "\n",
    "            # Only retain incidents in the United States\n",
    "            # (Our scope is City of Chicago, so keeping all of USA should be sufficient)\n",
    "            if \"United States\" in incident_info['incident_address_info']['incident_address']:\n",
    "                \n",
    "                # Only retain incidents in Illinois\n",
    "                if \"Illinois\" in incident_info['incident_address_info']['incident_address']:\n",
    "\n",
    "                    # Append this Illinois, USA incident to the list\n",
    "                    incident_list.append( incident_info )\n",
    "                    n_incidents += 1\n",
    "\n",
    "                    # Print a progress message\n",
    "                    # print(f\">> Added incident #{n_incidents}: {incident_info['incident_title']}\")\n",
    "\n",
    "            #DEBUG ****************************************\n",
    "            #if n_incidents > 3:\n",
    "            #    break\n",
    "\n",
    "    except TypeError:\n",
    "        # If an iterable is not provided in \"results\", then fail gracefully\n",
    "        pass\n",
    "            \n",
    "            \n",
    "    # Return the list of dictionaries with USA incident info\n",
    "    return incident_list\n",
    "\n",
    "\n",
    "\n",
    "# ************************************************************************************\n",
    "# FUNCTION: scrape_iwp()\n",
    "# This function accepts an HTML string from an\n",
    "# IWP website page that contains multiple incidents.\n",
    "# It then loops through the incidents on the page and the uses parse_one_incident()\n",
    "# function to grab the relevant incident info from the page.\n",
    "#\n",
    "# NOTE: The incidents are filtered to keep only those that occurred in the USA\n",
    "# since our project is focused on Chicago, IL.\n",
    "#\n",
    "# Arguments:\n",
    "#    a_startpage: Optional argument specifying the website page at which to start scraping\n",
    "#    a_pagecount: Optional argument specifying how many pages to scrape\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A list of dictionaries of USA incident information\n",
    "\n",
    "def scrape_iwp(a_startpage=1, a_pagecount=20000):\n",
    "\n",
    "    # Initialize PyMongo to work with MongoDBs\n",
    "    conn = 'mongodb://localhost:27017'\n",
    "    client = pymongo.MongoClient(conn)\n",
    "\n",
    "    # Define database and collection\n",
    "    db = client.etl_db\n",
    "\n",
    "    # Setup the splinter Browser\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    # url_iwp = 'https://iwaspoisoned.com'\n",
    "    # UPDATE: Added the \"?page=\" to restart scraping on pages not already obtained\n",
    "    \n",
    "    url_iwp = 'https://iwaspoisoned.com/?page=' + str(a_startpage)\n",
    "\n",
    "    # Visit the IWP page\n",
    "    browser.visit( url_iwp )\n",
    "\n",
    "    # Extract incidents from multiple pages\n",
    "    page_target = int(a_pagecount)\n",
    "\n",
    "    # How long to wait between pages to avoid triggering issues on website\n",
    "    page_wait = 2\n",
    "\n",
    "    # Count the number of pages visited\n",
    "    n_pages = 0\n",
    "\n",
    "    # Loop until no more pages or until page target is reached\n",
    "    full_incident_list = []\n",
    "    for j in range(page_target):\n",
    "        # Get a page full of incidents from the USA\n",
    "        i_list = parse_incident_page(browser.html)\n",
    "        n_pages += 1\n",
    "\n",
    "        # Add this list of incidents to a running list\n",
    "        # full_incident_list.extend(i_list)\n",
    "\n",
    "        # Add this list of incidents to the Mongo database\n",
    "        \n",
    "        try:\n",
    "            # Attempt the insert\n",
    "            insert_results = db.iwp.insert_many(i_list)\n",
    "            \n",
    "            # Print a progress marker\n",
    "            print(f\"Page {n_pages} of {a_pagecount}: {len(insert_results.inserted_ids)} of {len(i_list)} incidents added to DB. Total incidents: {db.iwp.count_documents({})}\")\n",
    "\n",
    "        except TypeError:\n",
    "            # It's possible the incident list was empty, which could trigger a TypeError.\n",
    "            # This is the case since it is being filtered for only Illinois, USA incidents\n",
    "            print(f\">> Page {n_pages}: No incidents captured\")\n",
    "        \n",
    "        # Check to see if a hyperlink with attribute 'rel' = 'next' is present\n",
    "        soup_thispage = BeautifulSoup(browser.html, 'lxml')\n",
    "        next_tag = soup_thispage.find('a', {'rel' : 'next'})\n",
    "\n",
    "        if next_tag:\n",
    "            # Ok, there is a next page - get the hyperlink\n",
    "            # print(f\"DEBUG: Going to next page (next_tag = '{next_tag}' \")\n",
    "            try:\n",
    "                next_page_url = next_tag['href']\n",
    "\n",
    "                # Wait for a specified number of seconds\n",
    "                time.sleep(page_wait)\n",
    "\n",
    "                # Click it!\n",
    "                browser.click_link_by_href(next_page_url)\n",
    "\n",
    "                #DEBUG ****************************************\n",
    "                # if n_pages > 3:\n",
    "                #    break\n",
    "\n",
    "            # If KeyError occurs, then this tag has no html link for some reason\n",
    "            except KeyError:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            # No more pages - break out of this loop\n",
    "            break\n",
    "    \n",
    "    # Close the Browser\n",
    "    browser.quit()\n",
    "            \n",
    "    # Return the number of pages scraped\n",
    "    return n_pages\n",
    "\n",
    "\n",
    "# EXAMPLE:\n",
    "# Command to Start at Page 1 of iwaspoisoned.com and Scrape 10 Pages,\n",
    "# only keeping Incidents that occurred in Illinois, USA\n",
    "#\n",
    "# In a _separate_ Python file, include the code below:\n",
    "\n",
    "#*******************************************************************************\n",
    "# Import ETL Scraper function `scrape_iwp` from the local file `etl_scrape_iwp`\n",
    "# from etl_scrape_iwp import scrape_iwp\n",
    "#\n",
    "# Use the function to scape pages\n",
    "# pages_scraped = scrape_iwp(1, 10)\n",
    "#*******************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform - Technical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load - Technical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
