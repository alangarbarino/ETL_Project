{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data from iwaspoisoned.com website using web scraping.\n",
    "# Then populate the information in a MongoDB\n",
    "# (to facilitate teaming, export the MongoDB to a JSON file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pprint import pprint\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.etl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the splinter Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url_iwp = 'https://iwaspoisoned.com'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url_iwp)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"col-md-6 report-first-box\">\n",
      "<p class=\"report-date\">Feb 17 2019 8:20pm</p>\n",
      "<a href=\"https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll\" title=\"Chick-fil-A, North Fairfield Road, Beavercreek, OH, USA - Got Food Poisoning? Report it now\">\n",
      "<h3 class=\"report-box-h\">Chick-fil-A, North Fairfield Road, Beavercreek, OH, USA</h3></a>\n",
      "<p class=\"report-tag\">Symptoms:\n",
      "                                                        Diarrhea,\n",
      "                                                        Fever,\n",
      "                                                        Nausea,\n",
      "                                                        Vomiting\n",
      "                            </p>\n",
      "<p class=\"report-tag\">Report Type: Food Poisoning</p>\n",
      "</div>\n",
      ">>> Incident Date: Feb 17 2019 8:20pm\n",
      ">>> Incident Title: Chick-fil-A, North Fairfield Road, Beavercreek, OH, USA - Got Food Poisoning? Report it now\n",
      ">>> Incident URL: https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll\n",
      ">>> Incident Report Type: Food Poisoning\n",
      ">>> Incident Symptoms: ['Diarrhea', 'Fever', 'Nausea', 'Vomiting']\n",
      ">>> Incident Misc Info: \n",
      "----------------------------------------\n",
      ">>> Description: I bought 2 sandwiches and french fries for my two boys in the evening on Feb 16,2019. My oldest one ate all of sandwich, but my youngest did not like it. So my husband and I shared the rest sandwich. After eating, my oldest boy felt uncomfortable. At First, I though he ate too fast. Then he started to vomiting almost for the whole night. Even badly, both my husband and I also vomited in the midnight. My boy also had a temperature. My husband had diarrhea. Except my little one, we are all sick. We report it.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# results are returned as an iterable list\n",
    "results = soup.find_all('div', class_='row div-report-box')\n",
    "\n",
    "# Loop through returned results\n",
    "for r in results:\n",
    "    # Get the primary incident report info from the main box\n",
    "    main_box = r.find('div', class_='report-first-box')\n",
    "    \n",
    "    # Date the incident occurred\n",
    "    incident_date = main_box.find('p', class_ = 'report-date').text.strip()\n",
    "    \n",
    "    # Title of the incident\n",
    "    incident_title = main_box.find('a')['title']\n",
    "    \n",
    "    # URL of the per-incident details\n",
    "    incident_url = main_box.find('a')['href']\n",
    "    \n",
    "    # Get the Symptoms\n",
    "    report_tags = main_box.find_all('p', class_ = 'report-tag')\n",
    "    \n",
    "    # Parse each report tag into its proper field\n",
    "    incident_symptoms = \"\"\n",
    "    incident_report_type = \"\"\n",
    "    incident_misc = \"\"\n",
    "    \n",
    "    for rt in report_tags:\n",
    "        # Get the text in this tag\n",
    "        rt_info = rt.text.strip()\n",
    "\n",
    "        # Symptoms\n",
    "        if \"Symptoms:\" in rt_info:\n",
    "            incident_symptoms = [ s.replace(',','') for s in rt_info[len(\"Symptoms: \"):].split() ]\n",
    "            \n",
    "        # Report Type\n",
    "        elif \"Report Type:\" in rt_info:\n",
    "            incident_report_type = rt_info[len(\"Report Type: \"):]\n",
    "        \n",
    "        # Ok... no idea what this report tag contains\n",
    "        else:\n",
    "            incident_misc = rt_info\n",
    "    \n",
    "    pprint(main_box)\n",
    "    print(f\">>> Incident Date: {incident_date}\")\n",
    "    print(f\">>> Incident Title: {incident_title}\")\n",
    "    print(f\">>> Incident URL: {incident_url}\")\n",
    "    print(f\">>> Incident Report Type: {incident_report_type}\")\n",
    "    print(f\">>> Incident Symptoms: {incident_symptoms}\")\n",
    "    print(f\">>> Incident Misc Info: {incident_misc}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Get the full description of the incident\n",
    "    # Assume this couple be populated in multiple paragraphs\n",
    "    desc_box = r.find('div', class_='report-second-box')\n",
    "    desc_list = desc_box.find_all('p')\n",
    "    desc_info = \"\"\n",
    "    for d in desc_list:\n",
    "        desc_info += d.text.strip()\n",
    "    \n",
    "    #pprint(descbox)\n",
    "    print(f\">>> Description: {desc_info}\")\n",
    "    print(\"-\"*40)\n",
    "    break;\n",
    "#    try:\n",
    "        # Dictionary to be inserted as a MongoDB document\n",
    "        #post = {\n",
    "        #    'title': title,\n",
    "        #    'price': price,\n",
    "        #    'url': link\n",
    "        #}\n",
    "\n",
    "        #db.iwp.insert_one(post)\n",
    "\n",
    "#    except Exception as e:\n",
    "#        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url_incident = 'https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url_incident)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# results are returned as an iterable list\n",
    "results = soup.find_all('div', class_='single-incident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Incident Detail - Address: 2360 North Fairfield Road, Beavercreek, 45431 Ohio, United States\n",
      ">>> Incident Detail - Address - Street1: 2360 North Fairfield Road\n",
      ">>> Incident Detail - Address - Street2: \n",
      ">>> Incident Detail - Address - Street3: \n",
      ">>> Incident Detail - Address - City: Beavercreek\n",
      ">>> Incident Detail - Address - State: Ohio\n",
      ">>> Incident Detail - Address - Zipcode: 45431\n",
      ">>> Incident Detail - Address - Country: United States\n",
      ">>> Incident Detail - Address: 2360 North Fairfield Road, Beavercreek, Ohio 45431, United States\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    # Incident detail page - title\n",
    "    # incident_detail_title = r.find('h1', class_='h1 post-title').text.strip()\n",
    "    \n",
    "    # Address\n",
    "    addr_info = r.find('span', class_='pl-1 py-0 text-muted').text.strip()\n",
    "    incident_address = ' '.join(addr_info.split())\n",
    "    \n",
    "    # Ok, we now have an address of the form:\n",
    "    # 2360 North Fairfield Road, Beavercreek, 45431 Ohio, United States\n",
    "    # But, would be nice to be able to break this up into\n",
    "    # individual components to facilitate address matching,\n",
    "    # Especially with the non-standard location of the zipcode\n",
    "    \n",
    "    if \"United States\" in incident_address:\n",
    "        # Create a list of address items\n",
    "        ai_list = incident_address.split(',')\n",
    "        \n",
    "        # Some items are mandatory and are at the end of the list of length = N\n",
    "        # N-1: Country e.g. \"United States\"\n",
    "        # N-2: Zipcode and State e.g. \"45431 Ohio\"\n",
    "        # N-3: City\n",
    "        # Other entries 0 to N-4: Street/Apt/etc.\n",
    "        \n",
    "        ai_size = len( ai_list )\n",
    "        # Country\n",
    "        incident_address_country = ai_list[ai_size-1].strip()\n",
    "        \n",
    "        # Split the next entry to get state and zipcode\n",
    "        zs_info = ai_list[ai_size-2].strip()\n",
    "        zs_delim = zs_info.find(' ')\n",
    "        # print(f\"zs_delim: {zs_delim}, zs_info: {zs_info}\")\n",
    "        incident_address_zipcode = zs_info[:zs_delim].strip()\n",
    "        incident_address_state = zs_info[zs_delim:].strip()\n",
    "        \n",
    "        # City\n",
    "        incident_address_city = ai_list[ai_size-3].strip()\n",
    "        \n",
    "        # Process up to 3 \"street\" type entries\n",
    "        incident_address_street = \"\"\n",
    "        incident_address_street2 = \"\"\n",
    "        incident_address_street3 = \"\"\n",
    "        \n",
    "        # print(f\"ai_size: {ai_size}\")\n",
    "        # First street address item\n",
    "        if ai_size >= 4:\n",
    "            incident_address_street = ai_list[0].strip()\n",
    "        \n",
    "        # Second street address item\n",
    "        if ai_size >= 5:\n",
    "            incident_address_street2 = ai_list[1].strip()\n",
    "\n",
    "        # Third street address item\n",
    "        if ai_size >= 6:\n",
    "            incident_address_street3 = ai_list[i].strip()\n",
    "            \n",
    "        # Reform the address - with standard formating\n",
    "        incident_address_standard = incident_address_street\n",
    "        if len(incident_address_street2) > 0:\n",
    "            incident_address_standard += \", \" + incident_address_street2\n",
    "        if len(incident_address_street3) > 0:\n",
    "            incident_address_standard += \", \" + incident_address_street3\n",
    "        if len(incident_address_city) > 0:\n",
    "            incident_address_standard += \", \" + incident_address_city\n",
    "        if len(incident_address_state) > 0:\n",
    "            incident_address_standard += \", \" + incident_address_state\n",
    "        if len(incident_address_zipcode) > 0:\n",
    "            incident_address_standard += \" \" + incident_address_zipcode\n",
    "        if len(incident_address_country) > 0:\n",
    "            incident_address_standard += \", \" + incident_address_country\n",
    "        \n",
    "    print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "    print(f\">>> Incident Detail - Address - Street1: {incident_address_street}\")\n",
    "    print(f\">>> Incident Detail - Address - Street2: {incident_address_street2}\")\n",
    "    print(f\">>> Incident Detail - Address - Street3: {incident_address_street3}\")\n",
    "    print(f\">>> Incident Detail - Address - City: {incident_address_city}\")\n",
    "    print(f\">>> Incident Detail - Address - State: {incident_address_state}\")\n",
    "    print(f\">>> Incident Detail - Address - Zipcode: {incident_address_zipcode}\")\n",
    "    print(f\">>> Incident Detail - Address - Country: {incident_address_country}\")\n",
    "    print(f\">>> Incident Detail - Address: {incident_address_standard}\")\n",
    "    print(\"-\"*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through returned results\n",
    "for r in results:\n",
    "    # Get the primary incident report info from the main box\n",
    "    main_box = r.find('div', class_='report-first-box')\n",
    "    \n",
    "    # Date the incident occurred\n",
    "    incident_date = main_box.find('p', class_ = 'report-date').text.strip()\n",
    "    \n",
    "    # Title of the incident\n",
    "    incident_title = main_box.find('a')['title']\n",
    "    \n",
    "    # URL of the per-incident details\n",
    "    incident_url = main_box.find('a')['href']\n",
    "    \n",
    "    # Get the Symptoms\n",
    "    report_tags = main_box.find_all('p', class_ = 'report-tag')\n",
    "    \n",
    "    # Parse each report tag into its proper field\n",
    "    incident_symptoms = \"\"\n",
    "    incident_report_type = \"\"\n",
    "    incident_misc = \"\"\n",
    "    \n",
    "    for rt in report_tags:\n",
    "        # Get the text in this tag\n",
    "        rt_info = rt.text.strip()\n",
    "\n",
    "        # Symptoms\n",
    "        if \"Symptoms:\" in rt_info:\n",
    "            incident_symptoms = [ s.replace(',','') for s in rt_info[len(\"Symptoms: \"):].split() ]\n",
    "            \n",
    "        # Report Type\n",
    "        elif \"Report Type:\" in rt_info:\n",
    "            incident_report_type = rt_info[len(\"Report Type: \"):]\n",
    "        \n",
    "        # Ok... no idea what this report tag contains\n",
    "        else:\n",
    "            incident_misc = rt_info\n",
    "    \n",
    "    pprint(main_box)\n",
    "    print(f\">>> Incident Date: {incident_date}\")\n",
    "    print(f\">>> Incident Title: {incident_title}\")\n",
    "    print(f\">>> Incident URL: {incident_url}\")\n",
    "    print(f\">>> Incident Report Type: {incident_report_type}\")\n",
    "    print(f\">>> Incident Symptoms: {incident_symptoms}\")\n",
    "    print(f\">>> Incident Misc Info: {incident_misc}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Get the full description of the incident\n",
    "    # Assume this couple be populated in multiple paragraphs\n",
    "    desc_box = r.find('div', class_='report-second-box')\n",
    "    desc_list = desc_box.find_all('p')\n",
    "    desc_info = \"\"\n",
    "    for d in desc_list:\n",
    "        desc_info += d.text.strip()\n",
    "    \n",
    "    #pprint(descbox)\n",
    "    print(f\">>> Description: {desc_info}\")\n",
    "    print(\"-\"*40)\n",
    "    break;\n",
    "#    try:\n",
    "        # Dictionary to be inserted as a MongoDB document\n",
    "        #post = {\n",
    "        #    'title': title,\n",
    "        #    'price': price,\n",
    "        #    'url': link\n",
    "        #}\n",
    "\n",
    "        #db.iwp.insert_one(post)\n",
    "\n",
    "#    except Exception as e:\n",
    "#        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display items in MongoDB collection\n",
    "#listings = db.items.find()\n",
    "\n",
    "#for listing in listings:\n",
    "#    print(listing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
