{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data from iwaspoisoned.com website using web scraping.\n",
    "# Then populate the information in a MongoDB\n",
    "# (to facilitate teaming, export the MongoDB to a JSON file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pprint import pprint\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.etl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the splinter Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: get_incident_detail()\n",
    "# This function accepts a url that points to a single incident detail page\n",
    "# and returns a dictionary with info from that page.\n",
    "#\n",
    "# Note: This function performs special parsing of addresses in the United States.\n",
    "# Addresses in the US will be parsed down to individual components\n",
    "# (street, street2, street3, city, state, zipcode, country)\n",
    "# In addition, these parsed components are then recombined\n",
    "# to form the full address in \"standard\" format\n",
    "# (i.e., Zipcode after the state instead of before state)\n",
    "#\n",
    "# Addresses for other countries are provided only as an address string\n",
    "#\n",
    "# Arguments:\n",
    "#    incident_detail_url: URL of the incident detail page\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def get_incident_detail(a_url):\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll'\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/subway-terminal-3-silver-dart-drive-toronto-on-canada-168642#emailscroll'\n",
    "    if len(a_url) == 0:\n",
    "        return None\n",
    "    \n",
    "    url_incident = a_url\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url_incident)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='single-incident')\n",
    "\n",
    "    for r in results:\n",
    "        # Incident detail page - title\n",
    "        # incident_detail_title = r.find('h1', class_='h1 post-title').text.strip()\n",
    "\n",
    "        # Address\n",
    "        addr_info = r.find('span', class_='pl-1 py-0 text-muted').text.strip()\n",
    "        incident_address = ' '.join(addr_info.split())\n",
    "\n",
    "        # Ok, we now have an address of the form:\n",
    "        # 2360 North Fairfield Road, Beavercreek, 45431 Ohio, United States\n",
    "        # But, would be nice to be able to break this up into\n",
    "        # individual components to facilitate address matching,\n",
    "        # Especially with the non-standard location of the zipcode\n",
    "\n",
    "        if \"United States\" in incident_address:\n",
    "            # Create a list of address items\n",
    "            ai_list = incident_address.split(',')\n",
    "\n",
    "            # Some items are mandatory and are at the end of the list of length = N\n",
    "            # N-1: Country e.g. \"United States\"\n",
    "            # N-2: Zipcode and State e.g. \"45431 Ohio\"\n",
    "            # N-3: City\n",
    "            # Other entries 0 to N-4: Street/Apt/etc.\n",
    "\n",
    "            ai_size = len( ai_list )\n",
    "            # Country\n",
    "            incident_address_country = ai_list[ai_size-1].strip()\n",
    "\n",
    "            # Split the next entry to get state and zipcode\n",
    "            zs_info = ai_list[ai_size-2].strip()\n",
    "            zs_delim = zs_info.find(' ')\n",
    "            # print(f\"zs_delim: {zs_delim}, zs_info: {zs_info}\")\n",
    "            incident_address_zipcode = zs_info[:zs_delim].strip()\n",
    "            incident_address_state = zs_info[zs_delim:].strip()\n",
    "\n",
    "            # City\n",
    "            incident_address_city = ai_list[ai_size-3].strip()\n",
    "\n",
    "            # Process up to 3 \"street\" type entries\n",
    "            incident_address_street = \"\"\n",
    "            incident_address_street2 = \"\"\n",
    "            incident_address_street3 = \"\"\n",
    "\n",
    "            # print(f\"ai_size: {ai_size}\")\n",
    "            # First street address item\n",
    "            if ai_size >= 4:\n",
    "                incident_address_street = ai_list[0].strip()\n",
    "\n",
    "            # Second street address item\n",
    "            if ai_size >= 5:\n",
    "                incident_address_street2 = ai_list[1].strip()\n",
    "\n",
    "            # Third street address item\n",
    "            if ai_size >= 6:\n",
    "                incident_address_street3 = ai_list[i].strip()\n",
    "\n",
    "            # Reform the address - with standard formating\n",
    "            incident_address_standard = incident_address_street\n",
    "            if len(incident_address_street2) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street2\n",
    "            if len(incident_address_street3) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street3\n",
    "            if len(incident_address_city) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_city\n",
    "            if len(incident_address_state) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_state\n",
    "            if len(incident_address_zipcode) > 0:\n",
    "                incident_address_standard += \" \" + incident_address_zipcode\n",
    "            if len(incident_address_country) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_country\n",
    "\n",
    "\n",
    "            #print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            #print(f\">>> Incident Detail - Address - Standard: {incident_address_standard}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street: {incident_address_street}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street2: {incident_address_street2}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street3: {incident_address_street3}\")\n",
    "            #print(f\">>> Incident Detail - Address - City: {incident_address_city}\")\n",
    "            #print(f\">>> Incident Detail - Address - State: {incident_address_state}\")\n",
    "            #print(f\">>> Incident Detail - Address - Zipcode: {incident_address_zipcode}\")\n",
    "            #print(f\">>> Incident Detail - Address - Country: {incident_address_country}\")\n",
    "            #print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "                'incident_address_standard': incident_address_standard,\n",
    "                'incident_address_street': incident_address_street,\n",
    "                'incident_address_street2': incident_address_street2,\n",
    "                'incident_address_street3': incident_address_street3,\n",
    "                'incident_address_city': incident_address_city,\n",
    "                'incident_address_state': incident_address_state,\n",
    "                'incident_address_zipcode': incident_address_zipcode,\n",
    "                'incident_address_country': incident_address_country\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Ok, for other countries, don't try to parse the incident_address\n",
    "            # print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            # print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "            }\n",
    "\n",
    "        # pprint(detail_post_item)\n",
    "        \n",
    "        return detail_post_item\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: parse_one_incident()\n",
    "# This function accepts a Beautiful Soup object that contains a single incident\n",
    "# and returns a dictionary with info for that incident.\n",
    "# This includes a call to the get_incident_detail() function,\n",
    "# which gets needed information from the detail page for this incident\n",
    "#\n",
    "# Arguments:\n",
    "#    a_bsobj: A Beautiful Soup object containing a single incident\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def parse_one_incident(a_bsobj):\n",
    "    \n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    r = a_bsobj\n",
    "\n",
    "    # Get the primary incident report info from the main box\n",
    "    main_box = r.find('div', class_='report-first-box')\n",
    "    \n",
    "    # Date the incident occurred\n",
    "    try:\n",
    "        incident_date = main_box.find('p', class_ = 'report-date').text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        incident_date = \"\"\n",
    "        \n",
    "\n",
    "    # Title of the incident\n",
    "    try:\n",
    "        incident_title = main_box.find('a')['title']\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_title = \"\"\n",
    "    \n",
    "    # Remove the tag phrase from the title if it's present\n",
    "    if \"- Got Food Poisoning? Report it now\" in incident_title:\n",
    "        i_delim = incident_title.find(\"- Got Food Poisoning? Report it now\")\n",
    "        incident_title = incident_title[:i_delim].strip()\n",
    "\n",
    "    # URL of the per-incident details\n",
    "    try:\n",
    "        incident_url = main_box.find('a')['href'].strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_url = \"\"\n",
    "\n",
    "    # Get the Symptoms\n",
    "    report_tags = main_box.find_all('p', class_ = 'report-tag')\n",
    "\n",
    "    # Parse each report tag into its proper field\n",
    "    incident_symptoms = \"\"\n",
    "    incident_report_type = \"\"\n",
    "    incident_misc = \"\"\n",
    "\n",
    "    for rt in report_tags:\n",
    "        # Get the text in this tag\n",
    "        rt_info = rt.text.strip()\n",
    "\n",
    "        # Symptoms\n",
    "        if \"Symptoms:\" in rt_info:\n",
    "            incident_symptoms = [ s.replace(',','') for s in rt_info[len(\"Symptoms: \"):].split() ]\n",
    "\n",
    "        # Report Type\n",
    "        elif \"Report Type:\" in rt_info:\n",
    "            incident_report_type = rt_info[len(\"Report Type: \"):]\n",
    "\n",
    "        # Ok... no idea what this report tag contains\n",
    "        else:\n",
    "            incident_misc = rt_info\n",
    "\n",
    "    #pprint(main_box)\n",
    "    #print(f\">>> Incident Date: {incident_date}\")\n",
    "    #print(f\">>> Incident Title: {incident_title}\")\n",
    "    #print(f\">>> Incident URL: {incident_url}\")\n",
    "    #print(f\">>> Incident Report Type: {incident_report_type}\")\n",
    "    #print(f\">>> Incident Symptoms: {incident_symptoms}\")\n",
    "    #print(f\">>> Incident Misc Info: {incident_misc}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Get the full description of the incident\n",
    "    # Assume this couple be populated in multiple paragraphs\n",
    "    desc_box = r.find('div', class_='report-second-box')\n",
    "    desc_list = desc_box.find_all('p')\n",
    "    incident_description = \"\"\n",
    "    for d in desc_list:\n",
    "        incident_description += d.text.strip()\n",
    "\n",
    "    #pprint(descbox)\n",
    "    #print(f\">>> Description: {incident_description}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Go to the detail page to get the one piece of info we\n",
    "    # need that's not on the main page - the address!\n",
    "    incident_address_info = get_incident_detail(incident_url)\n",
    "\n",
    "    # Place all this good info into a dictionary\n",
    "    post_item = {\n",
    "        'incident_title': incident_title,\n",
    "        'incident_date': incident_date,\n",
    "        'incident_url': incident_url,\n",
    "        'incident_report_type': incident_report_type,\n",
    "        'incident_symptoms': incident_symptoms,\n",
    "        'incident_misc': incident_misc,\n",
    "        'incident_address_info': incident_address_info,\n",
    "        'incident_description': incident_description\n",
    "    }\n",
    "    #pprint(post_item)\n",
    "\n",
    "    return post_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: parse_incident_page()\n",
    "# This function accepts an HTML string from an\n",
    "# IWP website page that contains multiple incidents.\n",
    "# It then loops through the incidents on the page and the uses parse_one_incident()\n",
    "# function to grab the relevant incident info from the page.\n",
    "#\n",
    "# NOTE: The incidents are filtered to keep only those that occurred in the USA\n",
    "# since our project is focused on Chicago, IL.\n",
    "#\n",
    "# Arguments:\n",
    "#    a_html: A string of HTML content containing multiple incidents\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A list of dictionaries of USA incident information\n",
    "\n",
    "def parse_incident_page(a_html):\n",
    "    \n",
    "    # Do a basic check\n",
    "    if len(a_html) == 0:\n",
    "        return None\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(a_html, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='row div-report-box')\n",
    "\n",
    "    # Keep track of how many entries we've added\n",
    "    n_incidents = 0\n",
    "\n",
    "    # Get info for all of the incidents on this page\n",
    "    incident_list = []\n",
    "    try:\n",
    "        for r in results:\n",
    "\n",
    "            # Parse this incident\n",
    "            incident_info = parse_one_incident(r)\n",
    "            #pprint(incident_info)\n",
    "\n",
    "            # Only retain incidents in the United States\n",
    "            # (Our scope is City of Chicago, so keeping all of USA should be sufficient)\n",
    "            if \"United States\" in incident_info['incident_address_info']['incident_address']:\n",
    "                # Append this USA incident to the list\n",
    "                incident_list.append( incident_info )\n",
    "                n_incidents += 1\n",
    "\n",
    "                # Print a progress message\n",
    "                # print(f\">> Added incident #{n_incidents}: {incident_info['incident_title']}\")\n",
    "\n",
    "            #DEBUG ****************************************\n",
    "            #if n_incidents > 3:\n",
    "            #    break\n",
    "\n",
    "    except TypeError:\n",
    "        # If an iterable is not provided in \"results\", then fail gracefully\n",
    "        pass\n",
    "            \n",
    "            \n",
    "    # Return the list of dictionaries with USA incident info\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 6 of 6 incidents added to DB. Total incidents: 6\n",
      "Page 2: 9 of 9 incidents added to DB. Total incidents: 15\n",
      "Page 3: 9 of 9 incidents added to DB. Total incidents: 24\n"
     ]
    }
   ],
   "source": [
    "# URL of page to be scraped\n",
    "url_iwp = 'https://iwaspoisoned.com'\n",
    "\n",
    "# Visit the IWP page\n",
    "browser.visit( url_iwp )\n",
    "\n",
    "# Extract incidents from multiple pages\n",
    "page_target = 20000\n",
    "\n",
    "# How long to wait between pages to avoid triggering issues on website\n",
    "page_wait = 2\n",
    "\n",
    "# Count the number of pages visited\n",
    "n_pages = 0\n",
    "\n",
    "# Loop until no more pages or until page target is reached\n",
    "full_incident_list = []\n",
    "for j in range(page_target):\n",
    "    # Get a page full of incidents from the USA\n",
    "    i_list = parse_incident_page(browser.html)\n",
    "    n_pages += 1\n",
    "    \n",
    "    # Add this list of incidents to a running list\n",
    "    # full_incident_list.extend(i_list)\n",
    "    \n",
    "    # Add this list of incidents to the Mongo database\n",
    "    # update_results = db.iwp.update_many({}, i_list, upsert=True)\n",
    "    insert_results = db.iwp.insert_many(i_list)\n",
    "        \n",
    "    # Print a progress marker\n",
    "    try:\n",
    "        print(f\"Page {n_pages}: {len(insert_results.inserted_ids)} of {len(i_list)} incidents added to DB. Total incidents: {db.iwp.count_documents({})}\")\n",
    "\n",
    "    except TypeError:\n",
    "        print(f\">> Page {n_pages}: TypeError\")\n",
    "    \n",
    "    # Check to see if a hyperlink with attribute 'rel' = 'next' is present\n",
    "    soup_thispage = BeautifulSoup(browser.html, 'lxml')\n",
    "    next_tag = soup_thispage.find('a', {'rel' : 'next'})\n",
    "        \n",
    "    if next_tag:\n",
    "        # Ok, there is a next page - get the hyperlink\n",
    "        try:\n",
    "            next_page_url = next_tag['href']\n",
    "    \n",
    "            # Wait for a specified number of seconds\n",
    "            time.sleep(page_wait)\n",
    "\n",
    "            # Click it!\n",
    "            browser.click_link_by_href(next_page_url)\n",
    "\n",
    "            #DEBUG ****************************************\n",
    "            # if n_pages > 3:\n",
    "            #    break\n",
    "            \n",
    "        # If KeyError occurs, then this tag has no html link for some reason\n",
    "        except KeyError:\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        # No more pages - break out of this loop\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display items in MongoDB collection\n",
    "#all_incidents = db.iwp.find()\n",
    "\n",
    "#j=0\n",
    "#for i in all_incidents:\n",
    "#    print(f\"{j}: {i['incident_title']}\")\n",
    "#    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
