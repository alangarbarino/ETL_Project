{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data from iwaspoisoned.com website using web scraping.\n",
    "# Then populate the information in a MongoDB\n",
    "# (to facilitate teaming, export the MongoDB to a JSON file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pprint import pprint\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.etl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the splinter Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: get_incident_detail()\n",
    "# This function accepts a url that points to a single incident detail page\n",
    "# and returns a dictionary with info from that page.\n",
    "#\n",
    "# Note: This function performs special parsing of addresses in the United States.\n",
    "# Addresses in the US will be parsed down to individual components\n",
    "# (street, street2, street3, city, state, zipcode, country)\n",
    "# In addition, these parsed components are then recombined\n",
    "# to form the full address in \"standard\" format\n",
    "# (i.e., Zipcode after the state instead of before state)\n",
    "#\n",
    "# Addresses for other countries are provided only as an address string\n",
    "#\n",
    "# Arguments:\n",
    "#    incident_detail_url: URL of the incident detail page\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def get_incident_detail(a_url):\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/chick-fil-a-north-fairfield-road-beavercreek-oh-usa-168576#emailscroll'\n",
    "    # url_incident = 'https://iwaspoisoned.com/incident/subway-terminal-3-silver-dart-drive-toronto-on-canada-168642#emailscroll'\n",
    "    if len(a_url) == 0:\n",
    "        return None\n",
    "    \n",
    "    url_incident = a_url\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url_incident)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='single-incident')\n",
    "\n",
    "    for r in results:\n",
    "        # Incident detail page - title\n",
    "        # incident_detail_title = r.find('h1', class_='h1 post-title').text.strip()\n",
    "\n",
    "        # Address\n",
    "        try:\n",
    "            addr_info = r.find('span', class_='pl-1 py-0 text-muted').text.strip()\n",
    "            incident_address = ' '.join(addr_info.split())\n",
    "            \n",
    "        except AttributeError:\n",
    "            addr_info = \"\"\n",
    "            incident_address = \"\"\n",
    "\n",
    "        # Ok, we now have an address of the form:\n",
    "        # 2360 North Fairfield Road, Beavercreek, 45431 Ohio, United States\n",
    "        # But, would be nice to be able to break this up into\n",
    "        # individual components to facilitate address matching,\n",
    "        # Especially with the non-standard location of the zipcode\n",
    "\n",
    "        if \"United States\" in incident_address:\n",
    "            # Create a list of address items\n",
    "            ai_list = incident_address.split(',')\n",
    "\n",
    "            # Some items are mandatory and are at the end of the list of length = N\n",
    "            # N-1: Country e.g. \"United States\"\n",
    "            # N-2: Zipcode and State e.g. \"45431 Ohio\"\n",
    "            # N-3: City\n",
    "            # Other entries 0 to N-4: Street/Apt/etc.\n",
    "\n",
    "            ai_size = len( ai_list )\n",
    "            # Country\n",
    "            incident_address_country = ai_list[ai_size-1].strip()\n",
    "\n",
    "            # Split the next entry to get state and zipcode\n",
    "            zs_info = ai_list[ai_size-2].strip()\n",
    "            zs_delim = zs_info.find(' ')\n",
    "            # print(f\"zs_delim: {zs_delim}, zs_info: {zs_info}\")\n",
    "            incident_address_zipcode = zs_info[:zs_delim].strip()\n",
    "            incident_address_state = zs_info[zs_delim:].strip()\n",
    "\n",
    "            # City\n",
    "            incident_address_city = ai_list[ai_size-3].strip()\n",
    "\n",
    "            # Process up to 3 \"street\" type entries\n",
    "            incident_address_street = \"\"\n",
    "            incident_address_street2 = \"\"\n",
    "            incident_address_street3 = \"\"\n",
    "\n",
    "            # print(f\"ai_size: {ai_size}\")\n",
    "            # First street address item\n",
    "            if ai_size >= 4:\n",
    "                incident_address_street = ai_list[0].strip()\n",
    "\n",
    "            # Second street address item\n",
    "            if ai_size >= 5:\n",
    "                incident_address_street2 = ai_list[1].strip()\n",
    "\n",
    "            # Third street address item\n",
    "            if ai_size >= 6:\n",
    "                incident_address_street3 = ai_list[i].strip()\n",
    "\n",
    "            # Reform the address - with standard formating\n",
    "            incident_address_standard = incident_address_street\n",
    "            if len(incident_address_street2) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street2\n",
    "            if len(incident_address_street3) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_street3\n",
    "            if len(incident_address_city) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_city\n",
    "            if len(incident_address_state) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_state\n",
    "            if len(incident_address_zipcode) > 0:\n",
    "                incident_address_standard += \" \" + incident_address_zipcode\n",
    "            if len(incident_address_country) > 0:\n",
    "                incident_address_standard += \", \" + incident_address_country\n",
    "\n",
    "\n",
    "            #print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            #print(f\">>> Incident Detail - Address - Standard: {incident_address_standard}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street: {incident_address_street}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street2: {incident_address_street2}\")\n",
    "            #print(f\">>> Incident Detail - Address - Street3: {incident_address_street3}\")\n",
    "            #print(f\">>> Incident Detail - Address - City: {incident_address_city}\")\n",
    "            #print(f\">>> Incident Detail - Address - State: {incident_address_state}\")\n",
    "            #print(f\">>> Incident Detail - Address - Zipcode: {incident_address_zipcode}\")\n",
    "            #print(f\">>> Incident Detail - Address - Country: {incident_address_country}\")\n",
    "            #print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "                'incident_address_standard': incident_address_standard,\n",
    "                'incident_address_street': incident_address_street,\n",
    "                'incident_address_street2': incident_address_street2,\n",
    "                'incident_address_street3': incident_address_street3,\n",
    "                'incident_address_city': incident_address_city,\n",
    "                'incident_address_state': incident_address_state,\n",
    "                'incident_address_zipcode': incident_address_zipcode,\n",
    "                'incident_address_country': incident_address_country\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Ok, for other countries, don't try to parse the incident_address\n",
    "            # print(f\">>> Incident Detail - Address: {incident_address}\")\n",
    "            # print(\"-\"*40)\n",
    "\n",
    "            # Place all this good info into a dictionary\n",
    "            detail_post_item = {\n",
    "                'incident_address': incident_address,\n",
    "            }\n",
    "\n",
    "        # pprint(detail_post_item)\n",
    "        \n",
    "        return detail_post_item\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: parse_one_incident()\n",
    "# This function accepts a Beautiful Soup object that contains a single incident\n",
    "# and returns a dictionary with info for that incident.\n",
    "# This includes a call to the get_incident_detail() function,\n",
    "# which gets needed information from the detail page for this incident\n",
    "#\n",
    "# Arguments:\n",
    "#    a_bsobj: A Beautiful Soup object containing a single incident\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A dictionary containing the incident detail page info\n",
    "\n",
    "def parse_one_incident(a_bsobj):\n",
    "    \n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    r = a_bsobj\n",
    "\n",
    "    # Get the primary incident report info from the main box\n",
    "    main_box = r.find('div', class_='report-first-box')\n",
    "    \n",
    "    # Date the incident occurred\n",
    "    try:\n",
    "        incident_date = main_box.find('p', class_ = 'report-date').text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        incident_date = \"\"\n",
    "        \n",
    "\n",
    "    # Title of the incident\n",
    "    try:\n",
    "        incident_title = main_box.find('a')['title']\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_title = \"\"\n",
    "    \n",
    "    # Remove the tag phrase from the title if it's present\n",
    "    if \"- Got Food Poisoning? Report it now\" in incident_title:\n",
    "        i_delim = incident_title.find(\"- Got Food Poisoning? Report it now\")\n",
    "        incident_title = incident_title[:i_delim].strip()\n",
    "\n",
    "    # URL of the per-incident details\n",
    "    try:\n",
    "        incident_url = main_box.find('a')['href'].strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        incident_url = \"\"\n",
    "\n",
    "    # Get the Symptoms\n",
    "    report_tags = main_box.find_all('p', class_ = 'report-tag')\n",
    "\n",
    "    # Parse each report tag into its proper field\n",
    "    incident_symptoms = \"\"\n",
    "    incident_report_type = \"\"\n",
    "    incident_misc = \"\"\n",
    "\n",
    "    for rt in report_tags:\n",
    "        # Get the text in this tag\n",
    "        rt_info = rt.text.strip()\n",
    "\n",
    "        # Symptoms\n",
    "        if \"Symptoms:\" in rt_info:\n",
    "            incident_symptoms = [ s.replace(',','') for s in rt_info[len(\"Symptoms: \"):].split() ]\n",
    "\n",
    "        # Report Type\n",
    "        elif \"Report Type:\" in rt_info:\n",
    "            incident_report_type = rt_info[len(\"Report Type: \"):]\n",
    "\n",
    "        # Ok... no idea what this report tag contains\n",
    "        else:\n",
    "            incident_misc = rt_info\n",
    "\n",
    "    #pprint(main_box)\n",
    "    #print(f\">>> Incident Date: {incident_date}\")\n",
    "    #print(f\">>> Incident Title: {incident_title}\")\n",
    "    #print(f\">>> Incident URL: {incident_url}\")\n",
    "    #print(f\">>> Incident Report Type: {incident_report_type}\")\n",
    "    #print(f\">>> Incident Symptoms: {incident_symptoms}\")\n",
    "    #print(f\">>> Incident Misc Info: {incident_misc}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Get the full description of the incident\n",
    "    # Assume this couple be populated in multiple paragraphs\n",
    "    desc_box = r.find('div', class_='report-second-box')\n",
    "    desc_list = desc_box.find_all('p')\n",
    "    incident_description = \"\"\n",
    "    for d in desc_list:\n",
    "        incident_description += d.text.strip()\n",
    "\n",
    "    #pprint(descbox)\n",
    "    #print(f\">>> Description: {incident_description}\")\n",
    "    #print(\"-\"*40)\n",
    "\n",
    "    # Go to the detail page to get the one piece of info we\n",
    "    # need that's not on the main page - the address!\n",
    "    incident_address_info = get_incident_detail(incident_url)\n",
    "\n",
    "    # Place all this good info into a dictionary\n",
    "    post_item = {\n",
    "        'incident_title': incident_title,\n",
    "        'incident_date': incident_date,\n",
    "        'incident_url': incident_url,\n",
    "        'incident_report_type': incident_report_type,\n",
    "        'incident_symptoms': incident_symptoms,\n",
    "        'incident_misc': incident_misc,\n",
    "        'incident_address_info': incident_address_info,\n",
    "        'incident_description': incident_description\n",
    "    }\n",
    "    #pprint(post_item)\n",
    "\n",
    "    return post_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION: parse_incident_page()\n",
    "# This function accepts an HTML string from an\n",
    "# IWP website page that contains multiple incidents.\n",
    "# It then loops through the incidents on the page and the uses parse_one_incident()\n",
    "# function to grab the relevant incident info from the page.\n",
    "#\n",
    "# NOTE: The incidents are filtered to keep only those that occurred in the USA\n",
    "# since our project is focused on Chicago, IL.\n",
    "#\n",
    "# Arguments:\n",
    "#    a_html: A string of HTML content containing multiple incidents\n",
    "#\n",
    "# Returns:\n",
    "#    retval: A list of dictionaries of USA incident information\n",
    "\n",
    "def parse_incident_page(a_html):\n",
    "    \n",
    "    # Do a basic check\n",
    "    if len(a_html) == 0:\n",
    "        return None\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(a_html, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    # results are returned as an iterable list\n",
    "    results = soup.find_all('div', class_='row div-report-box')\n",
    "\n",
    "    # Keep track of how many entries we've added\n",
    "    n_incidents = 0\n",
    "\n",
    "    # Get info for all of the incidents on this page\n",
    "    incident_list = []\n",
    "    try:\n",
    "        for r in results:\n",
    "\n",
    "            # Parse this incident\n",
    "            incident_info = parse_one_incident(r)\n",
    "            #pprint(incident_info)\n",
    "\n",
    "            # Only retain incidents in the United States\n",
    "            # (Our scope is City of Chicago, so keeping all of USA should be sufficient)\n",
    "            if \"United States\" in incident_info['incident_address_info']['incident_address']:\n",
    "                # Append this USA incident to the list\n",
    "                incident_list.append( incident_info )\n",
    "                n_incidents += 1\n",
    "\n",
    "                # Print a progress message\n",
    "                # print(f\">> Added incident #{n_incidents}: {incident_info['incident_title']}\")\n",
    "\n",
    "            #DEBUG ****************************************\n",
    "            #if n_incidents > 3:\n",
    "            #    break\n",
    "\n",
    "    except TypeError:\n",
    "        # If an iterable is not provided in \"results\", then fail gracefully\n",
    "        pass\n",
    "            \n",
    "            \n",
    "    # Return the list of dictionaries with USA incident info\n",
    "    return incident_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 7 of 7 incidents added to DB. Total incidents: 176\n",
      "Page 2: 8 of 8 incidents added to DB. Total incidents: 184\n",
      "Page 3: 9 of 9 incidents added to DB. Total incidents: 193\n",
      "Page 4: 9 of 9 incidents added to DB. Total incidents: 202\n",
      "Page 5: 9 of 9 incidents added to DB. Total incidents: 211\n",
      "Page 6: 10 of 10 incidents added to DB. Total incidents: 221\n",
      "Page 7: 10 of 10 incidents added to DB. Total incidents: 231\n",
      "Page 8: 7 of 7 incidents added to DB. Total incidents: 238\n",
      "Page 9: 9 of 9 incidents added to DB. Total incidents: 247\n",
      "Page 10: 9 of 9 incidents added to DB. Total incidents: 256\n",
      "Page 11: 8 of 8 incidents added to DB. Total incidents: 264\n",
      "Page 12: 8 of 8 incidents added to DB. Total incidents: 272\n",
      "Page 13: 8 of 8 incidents added to DB. Total incidents: 280\n",
      "Page 14: 8 of 8 incidents added to DB. Total incidents: 288\n",
      "Page 15: 8 of 8 incidents added to DB. Total incidents: 296\n",
      "Page 16: 9 of 9 incidents added to DB. Total incidents: 305\n",
      "Page 17: 8 of 8 incidents added to DB. Total incidents: 313\n",
      "Page 18: 10 of 10 incidents added to DB. Total incidents: 323\n",
      "Page 19: 9 of 9 incidents added to DB. Total incidents: 332\n",
      "Page 20: 7 of 7 incidents added to DB. Total incidents: 339\n",
      "Page 21: 8 of 8 incidents added to DB. Total incidents: 347\n",
      "Page 22: 7 of 7 incidents added to DB. Total incidents: 354\n",
      "Page 23: 9 of 9 incidents added to DB. Total incidents: 363\n",
      "Page 24: 9 of 9 incidents added to DB. Total incidents: 372\n",
      "Page 25: 9 of 9 incidents added to DB. Total incidents: 381\n",
      "Page 26: 4 of 4 incidents added to DB. Total incidents: 385\n",
      "Page 27: 9 of 9 incidents added to DB. Total incidents: 394\n",
      "Page 28: 5 of 5 incidents added to DB. Total incidents: 399\n",
      "Page 29: 8 of 8 incidents added to DB. Total incidents: 407\n",
      "Page 30: 9 of 9 incidents added to DB. Total incidents: 416\n",
      "Page 31: 9 of 9 incidents added to DB. Total incidents: 425\n",
      "Page 32: 8 of 8 incidents added to DB. Total incidents: 433\n",
      "Page 33: 8 of 8 incidents added to DB. Total incidents: 441\n",
      "Page 34: 8 of 8 incidents added to DB. Total incidents: 449\n",
      "Page 35: 7 of 7 incidents added to DB. Total incidents: 456\n",
      "Page 36: 9 of 9 incidents added to DB. Total incidents: 465\n",
      "Page 37: 9 of 9 incidents added to DB. Total incidents: 474\n",
      "Page 38: 9 of 9 incidents added to DB. Total incidents: 483\n",
      "Page 39: 9 of 9 incidents added to DB. Total incidents: 492\n",
      "Page 40: 8 of 8 incidents added to DB. Total incidents: 500\n",
      "Page 41: 8 of 8 incidents added to DB. Total incidents: 508\n",
      "Page 42: 10 of 10 incidents added to DB. Total incidents: 518\n",
      "Page 43: 9 of 9 incidents added to DB. Total incidents: 527\n",
      "Page 44: 9 of 9 incidents added to DB. Total incidents: 536\n",
      "Page 45: 9 of 9 incidents added to DB. Total incidents: 545\n",
      "Page 46: 9 of 9 incidents added to DB. Total incidents: 554\n",
      "Page 47: 10 of 10 incidents added to DB. Total incidents: 564\n",
      "Page 48: 9 of 9 incidents added to DB. Total incidents: 573\n",
      "Page 49: 8 of 8 incidents added to DB. Total incidents: 581\n",
      "Page 50: 8 of 8 incidents added to DB. Total incidents: 589\n",
      "Page 51: 10 of 10 incidents added to DB. Total incidents: 599\n",
      "Page 52: 9 of 9 incidents added to DB. Total incidents: 608\n",
      "Page 53: 8 of 8 incidents added to DB. Total incidents: 616\n",
      "Page 54: 9 of 9 incidents added to DB. Total incidents: 625\n",
      "Page 55: 6 of 6 incidents added to DB. Total incidents: 631\n",
      "Page 56: 8 of 8 incidents added to DB. Total incidents: 639\n",
      "Page 57: 8 of 8 incidents added to DB. Total incidents: 647\n",
      "Page 58: 8 of 8 incidents added to DB. Total incidents: 655\n",
      "Page 59: 9 of 9 incidents added to DB. Total incidents: 664\n",
      "Page 60: 9 of 9 incidents added to DB. Total incidents: 673\n",
      "Page 61: 7 of 7 incidents added to DB. Total incidents: 680\n",
      "Page 62: 9 of 9 incidents added to DB. Total incidents: 689\n",
      "Page 63: 8 of 8 incidents added to DB. Total incidents: 697\n",
      "Page 64: 9 of 9 incidents added to DB. Total incidents: 706\n",
      "Page 65: 7 of 7 incidents added to DB. Total incidents: 713\n",
      "Page 66: 7 of 7 incidents added to DB. Total incidents: 720\n",
      "Page 67: 8 of 8 incidents added to DB. Total incidents: 728\n",
      "Page 68: 7 of 7 incidents added to DB. Total incidents: 735\n",
      "Page 69: 7 of 7 incidents added to DB. Total incidents: 742\n",
      "Page 70: 6 of 6 incidents added to DB. Total incidents: 748\n",
      "Page 71: 8 of 8 incidents added to DB. Total incidents: 756\n",
      "Page 72: 9 of 9 incidents added to DB. Total incidents: 765\n",
      "Page 73: 10 of 10 incidents added to DB. Total incidents: 775\n",
      "Page 74: 9 of 9 incidents added to DB. Total incidents: 784\n",
      "Page 75: 9 of 9 incidents added to DB. Total incidents: 793\n",
      "Page 76: 6 of 6 incidents added to DB. Total incidents: 799\n",
      "Page 77: 9 of 9 incidents added to DB. Total incidents: 808\n",
      "Page 78: 9 of 9 incidents added to DB. Total incidents: 817\n",
      "Page 79: 10 of 10 incidents added to DB. Total incidents: 827\n",
      "Page 80: 9 of 9 incidents added to DB. Total incidents: 836\n",
      "Page 81: 9 of 9 incidents added to DB. Total incidents: 845\n",
      "Page 82: 9 of 9 incidents added to DB. Total incidents: 854\n",
      "Page 83: 9 of 9 incidents added to DB. Total incidents: 863\n",
      "Page 84: 9 of 9 incidents added to DB. Total incidents: 872\n",
      "Page 85: 7 of 7 incidents added to DB. Total incidents: 879\n",
      "Page 86: 8 of 8 incidents added to DB. Total incidents: 887\n",
      "Page 87: 7 of 7 incidents added to DB. Total incidents: 894\n",
      "Page 88: 8 of 8 incidents added to DB. Total incidents: 902\n",
      "Page 89: 5 of 5 incidents added to DB. Total incidents: 907\n",
      "Page 90: 9 of 9 incidents added to DB. Total incidents: 916\n",
      "Page 91: 8 of 8 incidents added to DB. Total incidents: 924\n",
      "Page 92: 8 of 8 incidents added to DB. Total incidents: 932\n",
      "Page 93: 9 of 9 incidents added to DB. Total incidents: 941\n",
      "Page 94: 10 of 10 incidents added to DB. Total incidents: 951\n",
      "Page 95: 7 of 7 incidents added to DB. Total incidents: 958\n",
      "Page 96: 10 of 10 incidents added to DB. Total incidents: 968\n",
      "Page 97: 8 of 8 incidents added to DB. Total incidents: 976\n",
      "Page 98: 9 of 9 incidents added to DB. Total incidents: 985\n",
      "Page 99: 8 of 8 incidents added to DB. Total incidents: 993\n",
      "Page 100: 9 of 9 incidents added to DB. Total incidents: 1002\n",
      "Page 101: 7 of 7 incidents added to DB. Total incidents: 1009\n",
      "Page 102: 10 of 10 incidents added to DB. Total incidents: 1019\n",
      "Page 103: 7 of 7 incidents added to DB. Total incidents: 1026\n",
      "Page 104: 9 of 9 incidents added to DB. Total incidents: 1035\n",
      "Page 105: 10 of 10 incidents added to DB. Total incidents: 1045\n",
      "Page 106: 9 of 9 incidents added to DB. Total incidents: 1054\n",
      "Page 107: 9 of 9 incidents added to DB. Total incidents: 1063\n",
      "Page 108: 6 of 6 incidents added to DB. Total incidents: 1069\n",
      "Page 109: 6 of 6 incidents added to DB. Total incidents: 1075\n",
      "Page 110: 10 of 10 incidents added to DB. Total incidents: 1085\n",
      "Page 111: 10 of 10 incidents added to DB. Total incidents: 1095\n",
      "Page 112: 10 of 10 incidents added to DB. Total incidents: 1105\n",
      "Page 113: 9 of 9 incidents added to DB. Total incidents: 1114\n",
      "Page 114: 8 of 8 incidents added to DB. Total incidents: 1122\n",
      "Page 115: 9 of 9 incidents added to DB. Total incidents: 1131\n",
      "Page 116: 9 of 9 incidents added to DB. Total incidents: 1140\n",
      "Page 117: 9 of 9 incidents added to DB. Total incidents: 1149\n",
      "Page 118: 7 of 7 incidents added to DB. Total incidents: 1156\n"
     ]
    }
   ],
   "source": [
    "# URL of page to be scraped\n",
    "# url_iwp = 'https://iwaspoisoned.com'\n",
    "url_iwp = 'https://iwaspoisoned.com/?page=25#'\n",
    "\n",
    "# Visit the IWP page\n",
    "browser.visit( url_iwp )\n",
    "\n",
    "# Extract incidents from multiple pages\n",
    "page_target = 20000\n",
    "\n",
    "# How long to wait between pages to avoid triggering issues on website\n",
    "page_wait = 2\n",
    "\n",
    "# Count the number of pages visited\n",
    "n_pages = 0\n",
    "\n",
    "# Loop until no more pages or until page target is reached\n",
    "full_incident_list = []\n",
    "for j in range(page_target):\n",
    "    # Get a page full of incidents from the USA\n",
    "    i_list = parse_incident_page(browser.html)\n",
    "    n_pages += 1\n",
    "    \n",
    "    # Add this list of incidents to a running list\n",
    "    # full_incident_list.extend(i_list)\n",
    "    \n",
    "    # Add this list of incidents to the Mongo database\n",
    "    # update_results = db.iwp.update_many({}, i_list, upsert=True)\n",
    "    insert_results = db.iwp.insert_many(i_list)\n",
    "        \n",
    "    # Print a progress marker\n",
    "    try:\n",
    "        print(f\"Page {n_pages}: {len(insert_results.inserted_ids)} of {len(i_list)} incidents added to DB. Total incidents: {db.iwp.count_documents({})}\")\n",
    "\n",
    "    except TypeError:\n",
    "        print(f\">> Page {n_pages}: TypeError\")\n",
    "    \n",
    "    # Check to see if a hyperlink with attribute 'rel' = 'next' is present\n",
    "    soup_thispage = BeautifulSoup(browser.html, 'lxml')\n",
    "    next_tag = soup_thispage.find('a', {'rel' : 'next'})\n",
    "        \n",
    "    if next_tag:\n",
    "        # Ok, there is a next page - get the hyperlink\n",
    "        try:\n",
    "            next_page_url = next_tag['href']\n",
    "    \n",
    "            # Wait for a specified number of seconds\n",
    "            time.sleep(page_wait)\n",
    "\n",
    "            # Click it!\n",
    "            browser.click_link_by_href(next_page_url)\n",
    "\n",
    "            #DEBUG ****************************************\n",
    "            # if n_pages > 3:\n",
    "            #    break\n",
    "            \n",
    "        # If KeyError occurs, then this tag has no html link for some reason\n",
    "        except KeyError:\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        # No more pages - break out of this loop\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display items in MongoDB collection\n",
    "#all_incidents = db.iwp.find()\n",
    "\n",
    "#j=0\n",
    "#for i in all_incidents:\n",
    "#    print(f\"{j}: {i['incident_title']}\")\n",
    "#    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
